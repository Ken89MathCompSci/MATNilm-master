{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ken89MathCompSci/MATNilm-master/blob/master/MATNilm_base_19_September_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "K64qyQzOCYLW",
        "outputId": "babac4d9-6d74-4b35-ba88-d9617792a51c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/MATNilm-master'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXbzVdNKkD_",
        "outputId": "cb34d93f-a675-43ea-c1e5-e1a19db677ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'MATNilm-master/'\n",
            "/content/MATNilm-master\n"
          ]
        }
      ],
      "source": [
        "cd MATNilm-master/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGW_TXKxK9Ej",
        "outputId": "0b1da35f-7b38-4fdf-c42f-2132288a45a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-19 13:37:07,776 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-19 13:37:07,777 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='test', inputLength=864, outputLength=864, debug=False, dataAug=True, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, resume=True, checkpoint='All_best_onoff.ckpt', save_dir='/content/drive/MyDrive/MATNilm_checkpoints')\n",
            "2025-09-19 13:37:07,777 - root - INFO - loading data\n",
            "2025-09-19 13:37:07,808 - root - INFO - loading data finished\n",
            "2025-09-19 13:37:07,808 - root - INFO - Training size: 27937.\n",
            "2025-09-19 13:37:07,808 - root - INFO - Initialize model\n",
            "2025-09-19 13:37:08,055 - root - INFO - Model MAT\n",
            "2025-09-19 13:37:08,949 - root - INFO - Resuming training from checkpoint: history_model/test/s0/All_best_onoff.ckpt\n",
            "2025-09-19 13:37:09,099 - root - INFO - Model loaded\n",
            "2025-09-19 13:37:09,099 - root - INFO - Training start\n",
            "2025-09-19 13:37:09,108 - root - INFO - # of epoches: 0\n",
            "  0% 0/874 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:366: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
            "  return F.conv1d(\n",
            "  2% 18/874 [00:20<16:04,  1.13s/it]"
          ]
        }
      ],
      "source": [
        "# Write the content of cell 95b50e1b to main.py\n",
        "with open(\"main.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import copy\n",
        "import os\n",
        "import utils\n",
        "import argparse\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from custom_types import Basic, TrainConfig\n",
        "from modules import MATconv as MAT\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "\n",
        "def get_args():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--batch\", type=int, default=32, help=\"batch size\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout\")\n",
        "    parser.add_argument(\"--hidden\", type=int, default=32, help=\"encoder decoder hidden size\")\n",
        "    parser.add_argument(\"--logname\", action=\"store\", default='root', help=\"name for log\")\n",
        "    parser.add_argument(\"--subName\", action=\"store\", type=str, default='test', help=\"name of the directory of current run\")\n",
        "    parser.add_argument(\"--inputLength\", type=int, default=864, help=\"input length for the model\")\n",
        "    parser.add_argument(\"--outputLength\", type=int, default=864, help=\"output length for the model\")\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debug mode\")\n",
        "    parser.add_argument(\"--dataAug\", action=\"store_true\", help=\"data augmentation mode\")\n",
        "    parser.add_argument(\"--prob0\", type=float, default=0.3, help=\"augment probability for Dishwasher\")\n",
        "    parser.add_argument(\"--prob1\", type=float, default=0.6, help=\"weight\")\n",
        "    parser.add_argument(\"--prob2\", type=float, default=0.3, help=\"weight\")\n",
        "    parser.add_argument(\"--prob3\", type=float, default=0.3, help=\"weight\")\n",
        "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"resume training from checkpoint\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str, default=\"All_best_onoff.ckpt\", help=\"checkpoint file name to resume from\")\n",
        "    parser.add_argument(\"--save_dir\", type=str, default=\"./\", help=\"directory to save checkpoints\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def train(t_net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=200):\n",
        "    iter_loss = []\n",
        "    vali_loss = []\n",
        "    early_stopping_all = utils.EarlyStopping(logger, patience=30, verbose=True)\n",
        "\n",
        "    if config.dataAug:\n",
        "        sigClass = utils.sigGen(config)\n",
        "\n",
        "    path_all = os.path.join(modelDir, \"All_best_onoff.ckpt\")\n",
        "\n",
        "    for e_i in range(epo):\n",
        "\n",
        "        logger.info(f\"# of epoches: {e_i}\")\n",
        "        for t_i, (_, _, X_scaled, Y_scaled, Y_of) in enumerate(tqdm(train_Dataloader)):\n",
        "            if config.dataAug:\n",
        "                X_scaled, Y_scaled, Y_of = utils.dataAug(X_scaled.clone(), Y_scaled.clone(), Y_of.clone(), sigClass, config)\n",
        "\n",
        "            t_net.model_opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            X_scaled = X_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "            Y_scaled = Y_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "            Y_of = Y_of.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "\n",
        "            y_pred_dish_r, y_pred_dish_c = t_net.model(X_scaled)\n",
        "\n",
        "            loss_r = criterion[0](y_pred_dish_r,Y_scaled)\n",
        "            loss_c = criterion[1](y_pred_dish_c, Y_of)\n",
        "\n",
        "            loss=loss_r+loss_c\n",
        "            loss.backward()\n",
        "\n",
        "            t_net.model_opt.step()\n",
        "            iter_loss.append(loss.item())\n",
        "\n",
        "        epoch_losses = np.average(iter_loss)\n",
        "\n",
        "        logger.info(f\"Validation: \")\n",
        "        maeScore, y_vali_ori, y_vali_pred_d_update, _, _, _ = utils.evaluateResult(net, config, vali_Dataloader, logger)\n",
        "        val_loss = criterion[0](y_vali_ori, y_vali_pred_d_update)\n",
        "        logger.info(f\"Epoch {e_i:d}, train loss: {epoch_losses:3.3f}, val loss: {val_loss:3.3f}.\")\n",
        "        vali_loss.append(val_loss)\n",
        "\n",
        "        if e_i % 10 == 0:\n",
        "            checkpointName = os.path.join(config.save_dir, \"checkpoint_\" + str(e_i) + '.ckpt')\n",
        "            utils.saveModel(logger, net, checkpointName)\n",
        "\n",
        "        logger.info(f\"Early stopping overall: \")\n",
        "        early_stopping_all(np.mean(maeScore), net, path_all)\n",
        "        if early_stopping_all.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    net_all = copy.deepcopy(net)\n",
        "    checkpoint_all = torch.load(path_all, map_location=device)\n",
        "    utils.loadModel(logger, net_all, checkpoint_all)\n",
        "    net_all.model.eval()\n",
        "\n",
        "    return net_all\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    utils.mkdir(\"log/\" + args.subName)\n",
        "    logger = utils.setup_log(args.subName, args.logname)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using computation device: {device}\")\n",
        "    logger.info(args)\n",
        "    if args.debug:\n",
        "        epo = 2\n",
        "    else:\n",
        "        epo = 200\n",
        "\n",
        "    # splitLoss = False\n",
        "    # trainFull = True\n",
        "\n",
        "    # Dataloder\n",
        "    logger.info(f\"loading data\")\n",
        "    train_data, val_data, test_data = utils.data_loader(args)\n",
        "\n",
        "    logger.info(f\"loading data finished\")\n",
        "\n",
        "    config_dict = {\n",
        "        \"input_size\": 1,\n",
        "        \"batch_size\": args.batch,\n",
        "        \"hidden\": args.hidden,\n",
        "        \"lr\": args.lr,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"logname\": args.logname,\n",
        "        \"outputLength\": args.outputLength,\n",
        "        \"inputLength\" : args.inputLength,\n",
        "        \"subName\": args.subName,\n",
        "        \"dataAug\": args.dataAug,\n",
        "        \"prob0\": args.prob0,\n",
        "        \"prob1\": args.prob1,\n",
        "        \"prob2\": args.prob2,\n",
        "        \"prob3\": args.prob3,\n",
        "        \"save_dir\": args.save_dir,\n",
        "    }\n",
        "\n",
        "    config = TrainConfig.from_dict(config_dict)\n",
        "    modelDir = utils.mkdirectory(config.subName, saveModel=True)\n",
        "    joblib.dump(config, os.path.join(modelDir, \"config.pkl\"))\n",
        "\n",
        "\n",
        "    logger.info(f\"Training size: {train_data.cumulative_sizes[-1]:d}.\")\n",
        "\n",
        "    index = np.arange(0,train_data.cumulative_sizes[-1])\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(index)\n",
        "    train_Dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=train_subsampler,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    sampler = utils.testSampler(val_data.cumulative_sizes[-1], config.outputLength)\n",
        "    sampler_test = utils.testSampler(test_data.cumulative_sizes[-1], config.outputLength)\n",
        "\n",
        "    vali_Dataloader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    test_Dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler_test,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    logger.info(\"Initialize model\")\n",
        "    model = MAT(config).to(device)\n",
        "    logger.info(\"Model MAT\")\n",
        "\n",
        "    optim = optim.Adam(params=[p for p in model.parameters() if p.requires_grad], lr=config.lr)\n",
        "    net = Basic(model, optim)\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if args.resume:\n",
        "        checkpoint_path = os.path.join(modelDir, args.checkpoint)\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            logger.info(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "            net = utils.loadModel(logger, net, checkpoint)\n",
        "        else:\n",
        "            logger.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "            logger.info(\"Starting training from scratch\")\n",
        "\n",
        "    criterion_r = nn.MSELoss()\n",
        "    criterion_c = nn.BCELoss()\n",
        "    criterion = [criterion_r, criterion_c]\n",
        "\n",
        "    logger.info(\"Training start\")\n",
        "    net_all = train(net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=epo)\n",
        "    logger.info(\"Training end\")\n",
        "\n",
        "    logger.info(\"validation start\")\n",
        "    utils.evaluateResult(net_all, config, vali_Dataloader, logger)\n",
        "    logger.info(\"test start\")\n",
        "    utils.evaluateResult(net_all, config, test_Dataloader, logger)\n",
        "\"\"\")\n",
        "!python main.py --resume --dataAug --save_dir \"/content/drive/MyDrive/MATNilm_checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80b1ae1a"
      },
      "source": [
        "# Modify custom_types.py to include save_dir in TrainConfig.from_dict\n",
        "with open(\"custom_types.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import collections\n",
        "import typing\n",
        "\n",
        "class TrainConfig(typing.NamedTuple):\n",
        "\n",
        "    input_size: int\n",
        "    batch_size: int\n",
        "    hidden: int\n",
        "    lr: float\n",
        "    dropout: float\n",
        "    logname: str\n",
        "    outputLength: int\n",
        "    inputLength: int\n",
        "    subName: str\n",
        "    save_dir: str # Add save_dir here\n",
        "    debug: bool = False\n",
        "    dataAug: bool = False\n",
        "    prob0: float = 0.1\n",
        "    prob1: float = 0.1\n",
        "    prob2: float = 0.6\n",
        "    prob3: float = 0.6\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls,dikt):\n",
        "        input_size = dikt['input_size']\n",
        "        batch_size = dikt['batch_size']\n",
        "        hidden = dikt['hidden']\n",
        "        lr = dikt['lr']\n",
        "        dropout = dikt['dropout']\n",
        "        logname = dikt[\"logname\"]\n",
        "        outputLength = dikt[\"outputLength\"]\n",
        "        inputLength= dikt[\"inputLength\"]\n",
        "        subName = dikt['subName']\n",
        "        save_dir = dikt['save_dir'] # Get save_dir from dictionary\n",
        "        dataAug = dikt['dataAug']\n",
        "        prob0 = dikt['prob0']\n",
        "        prob1 = dikt['prob1']\n",
        "        prob2 = dikt['prob2']\n",
        "        prob3 = dikt['prob3']\n",
        "\n",
        "\n",
        "        return cls(input_size,batch_size,hidden,lr,dropout,\n",
        "                   logname,outputLength,inputLength,subName,save_dir,dataAug=dataAug, prob0=prob0, prob1=prob1, prob2=prob2, prob3=prob3)\n",
        "\n",
        "\n",
        "Basic = collections.namedtuple(\"NILM\", [\"model\", \"model_opt\"])\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d441db7"
      },
      "source": [
        "# Task\n",
        "Modify the `main.py` script to save checkpoints to a specified Google Drive directory and run the script with the necessary arguments to enable this functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1197e9"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to access files from the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27f106eb"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to access files from the Colab environment and then print the current working directory to confirm successful mounting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21795c15"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0556b962"
      },
      "source": [
        "import os\n",
        "print(os.listdir(\"/content/drive/MyDrive/MATNilm_checkpoints\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1gt6nzKJ_r_PvhrFrC7h8HsO_NaUCVe24",
      "authorship_tag": "ABX9TyNiu3SJOwbAUMMHVz1B0NM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}